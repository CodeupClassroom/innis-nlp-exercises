{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712bd79d",
   "metadata": {},
   "source": [
    "# NLP Modeling\n",
    "\n",
    "How do we quantify a document?\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Data Representation](#data-representation)\n",
    "    - [Bag of Words](#bag-of-words)\n",
    "    - [TF-IDF](#tf-idf)\n",
    "    - [Bag Of Ngrams](#bag-of-ngrams)\n",
    "- [Modeling](#modeling)\n",
    "    - [Modeling Results](#modeling-results)\n",
    "- [Next Steps](#next-steps)\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77207323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "def clean(text: str) -> list:\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = (text.encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split() # tokenization\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d09b64",
   "metadata": {},
   "source": [
    "## Data Representation\n",
    "\n",
    "Simple data for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b4c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    'Python is pretty cool',\n",
    "    'Python is a nice programming language with nice syntax',\n",
    "    'I think SQL is cool too',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fdb030",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778facc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bag_of_words = cv.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2721c0-901a-4639-8081-c7a76ff48bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f97319",
   "metadata": {},
   "source": [
    "Here `bag_of_words` is a **sparse matrix**. Usually you should keep it as such,\n",
    "but for demonstration we'll view the data within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef31e3a-5eeb-4394-89ad-8a190f2cd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccdb7a-45f6-49e4-82b2-55d880b9b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290cc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data)\n",
    "# Taking a look at the bag of words transformation for education and diagnostics.\n",
    "# In practice this is not necesssary and the resulting data might be to big to be reasonably helpful.\n",
    "bow = pd.DataFrame(bag_of_words.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8425d9-61a0-4e2f-9b60-03668ae00266",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18b2f9-5491-48fb-a9d0-068632bcff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.apply(lambda row: row / row.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929f4ac",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "- term frequency - inverse document frequency\n",
    "- $\\text{tf} \\times \\text{idf} = \\frac{\\text{tf}}{\\text{df}}$\n",
    "- a measure that helps identify how important a word is in a document\n",
    "- combination of how often a word appears in a document (**tf**) and how unqiue the word\n",
    "  is among documents (**idf**)\n",
    "- used by search engines\n",
    "- naturally helps filter out stopwords\n",
    "- tf is for a single document, idf is for a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c634645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bag_of_words = tfidf.fit_transform(data)\n",
    "\n",
    "pprint(data)\n",
    "pd.DataFrame(bag_of_words.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879a6e4",
   "metadata": {},
   "source": [
    "To get the idf score for each word (these aren't terribly usefule themselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73321fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(tfidf.get_feature_names(), tfidf.idf_))).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ba50b",
   "metadata": {},
   "source": [
    "### Bag Of Ngrams\n",
    "\n",
    "For either `CountVectorizer` or `TfidfVectorizer`, you can set the `ngram_range`\n",
    "parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e430d-9eb0-4293-9e85-f523c127e768",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "bag_of_words = cv.fit_transform(data)\n",
    "\n",
    "pprint(data)\n",
    "pd.DataFrame(bag_of_words.todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12896c9b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5172aa-73a8-459a-a3bd-68309de6a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df = pd.read_csv('spam_clean.csv')\n",
    "df['clean_text'] = df.text.apply(clean).apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823ff13-cdda-4faa-a516-6e9d9fcf1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67213d4-839f-454f-acb4-83fc6f1ca2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.clean_text\n",
    "y = df.label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7243917-b1a4-49c2-a87b-889e2e704a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X_bow = cv.fit_transform(X_train)\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_bow, y_train)\n",
    "\n",
    "tree.score(X_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32deec1e-9786-4349-b0d5-f87f6e066133",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(X_train)\n",
    "tree.fit(X_tfidf, y_train)\n",
    "tree.score(X_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5725ba1-404a-4e83-a419-6ca83f929092",
   "metadata": {},
   "source": [
    "Iterate:\n",
    "\n",
    "- try out the bag of ngrams\n",
    "- try out different ways of text prep (stem vs lemmatize)\n",
    "- etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whatever transformations we apply to X_train need to be applied to X_test\n",
    "tree.score(tfidf.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780da33c",
   "metadata": {},
   "source": [
    "### Modeling Results\n",
    "\n",
    "A super-useful feature of decision trees and linear models is that they do some\n",
    "built-in feature selection through the coefficeints or feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da113a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(zip(cv.get_feature_names_out(), tree.feature_importances_))).sort_values().tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e11613",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try other model types\n",
    "\n",
    "    [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "    ([`sklearn`\n",
    "    docs](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html))\n",
    "    is a very popular classifier for NLP tasks.\n",
    "\n",
    "- Look at other metrics, is accuracy the best choice here?\n",
    "\n",
    "- Try ngrams instead of single words\n",
    "\n",
    "- Try a combination of ngrams and words (`ngram_range=(1, 2)` for words and\n",
    "  bigrams)\n",
    "\n",
    "- Try using tf-idf instead of bag of words\n",
    "\n",
    "- Combine the top `n` performing words with the other features that you have\n",
    "  engineered (the `CountVectorizer` and `TfidfVectorizer` have a `vocabulary`\n",
    "  argument you can use to restrict the words used)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
